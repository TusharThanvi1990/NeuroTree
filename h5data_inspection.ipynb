{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8ebd70e-bd7f-4d43-be13-dc42080b0a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the file: ['evals', 'inputs']\n",
      "\n",
      "Inspecting dataset: evals\n",
      "Shape: (99086,)\n",
      "Data type: float64\n",
      "No NaN values found in the sample.\n",
      "No infinity values found in the sample.\n",
      "Data range: [0.0005527786369235996, 0.9994472213630764]\n",
      "'evals' values are within the expected [0, 1] range.\n",
      "Sample data from 'evals':\n",
      "[0.52061331 0.52560259 0.47065875 0.4725277  0.47626785]\n",
      "\n",
      "Inspecting dataset: inputs\n",
      "Shape: (99086, 8, 8, 19)\n",
      "Data type: float32\n",
      "No NaN values found in the sample.\n",
      "No infinity values found in the sample.\n",
      "Data range: [0.0, 1.0]\n",
      "'inputs' values are within the expected [0, 1] range.\n",
      "Sample data from 'inputs':\n",
      "[[[[0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 1. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 1. ... 1. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 1. ... 1. 0. 0.]\n",
      "   [0. 1. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]]\n",
      "\n",
      "  [[1. 0. 0. ... 1. 0. 0.]\n",
      "   [1. 0. 0. ... 1. 0. 0.]\n",
      "   [1. 0. 0. ... 1. 0. 0.]\n",
      "   ...\n",
      "   [1. 0. 0. ... 1. 0. 0.]\n",
      "   [1. 0. 0. ... 1. 0. 0.]\n",
      "   [1. 0. 0. ... 1. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 1. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 1. ... 1. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 1. ... 1. 0. 0.]\n",
      "   [0. 1. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]]\n",
      "\n",
      "  [[1. 0. 0. ... 1. 0. 0.]\n",
      "   [1. 0. 0. ... 1. 0. 0.]\n",
      "   [1. 0. 0. ... 1. 0. 0.]\n",
      "   ...\n",
      "   [1. 0. 0. ... 1. 0. 0.]\n",
      "   [1. 0. 0. ... 1. 0. 0.]\n",
      "   [1. 0. 0. ... 1. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]\n",
      "   [0. 0. 0. ... 1. 0. 0.]]]]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def inspect_hdf5_file(file_path):\n",
    "    \"\"\"\n",
    "    Thoroughly inspect the contents of an HDF5 file and check for null values, anomalies, and other potential issues.\n",
    "    \"\"\"\n",
    "    with h5py.File(file_path, 'r') as h5file:\n",
    "        print(\"Keys in the file:\", list(h5file.keys()))\n",
    "\n",
    "        for key in h5file.keys():\n",
    "            dataset = h5file[key]\n",
    "            print(f\"\\nInspecting dataset: {key}\")\n",
    "            print(f\"Shape: {dataset.shape}\")\n",
    "            print(f\"Data type: {dataset.dtype}\")\n",
    "\n",
    "            # Load a sample of the data (first 1000 items or all if less)\n",
    "            sample_size = min(1000, dataset.shape[0])\n",
    "            data_sample = dataset[:sample_size]\n",
    "\n",
    "            # Check for NaN values\n",
    "            nan_count = np.isnan(data_sample).sum()\n",
    "            if nan_count > 0:\n",
    "                print(f\"WARNING: {nan_count} NaN values found in the sample!\")\n",
    "            else:\n",
    "                print(\"No NaN values found in the sample.\")\n",
    "\n",
    "            # Check for infinity values\n",
    "            inf_count = np.isinf(data_sample).sum()\n",
    "            if inf_count > 0:\n",
    "                print(f\"WARNING: {inf_count} infinity values found in the sample!\")\n",
    "            else:\n",
    "                print(\"No infinity values found in the sample.\")\n",
    "\n",
    "            # Check data range\n",
    "            data_min = np.min(data_sample)\n",
    "            data_max = np.max(data_sample)\n",
    "            print(f\"Data range: [{data_min}, {data_max}]\")\n",
    "\n",
    "            if key == 'inputs':\n",
    "                # For 'inputs', check if values are within [0, 1]\n",
    "                if data_min < 0 or data_max > 1:\n",
    "                    print(\"WARNING: 'inputs' contains values outside the expected [0, 1] range!\")\n",
    "                else:\n",
    "                    print(\"'inputs' values are within the expected [0, 1] range.\")\n",
    "\n",
    "            elif key == 'evals':\n",
    "                # For 'evals', check if values are within [0, 1] (normalized evaluation scores)\n",
    "                if data_min < 0 or data_max > 1:\n",
    "                    print(\"WARNING: 'evals' contains values outside the expected [0, 1] range!\")\n",
    "                else:\n",
    "                    print(\"'evals' values are within the expected [0, 1] range.\")\n",
    "\n",
    "            elif key == 'moves':\n",
    "                # For 'moves', check if it's one-hot encoded (sum of each row should be 1)\n",
    "                row_sums = np.sum(data_sample, axis=1)\n",
    "                if not np.allclose(row_sums, 1):\n",
    "                    print(\"WARNING: 'moves' may not be properly one-hot encoded!\")\n",
    "                else:\n",
    "                    print(\"'moves' appears to be properly one-hot encoded.\")\n",
    "\n",
    "            # Check for constant values\n",
    "            if np.all(data_sample == data_sample[0]):\n",
    "                print(f\"WARNING: All values in '{key}' are constant!\")\n",
    "\n",
    "            # Print a small sample of the data\n",
    "            print(f\"Sample data from '{key}':\")\n",
    "            if data_sample.ndim == 1:\n",
    "                print(data_sample[:5])\n",
    "            else:\n",
    "                print(data_sample[:2])\n",
    "\n",
    "# Example usage\n",
    "h5_file_path = r\"Chess_dataset_final10_eval.h5\"\n",
    "inspect_hdf5_file(h5_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91b24592-5764-4061-be5a-c4594914f829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original inputs shape: (6804, 8, 8, 19)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 74\u001b[0m\n\u001b[0;32m     71\u001b[0m input_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAarti\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mChess_dataset4.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m output_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAarti\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mChess_data_augmented3.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 74\u001b[0m \u001b[43maugment_chess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 65\u001b[0m, in \u001b[0;36maugment_chess_dataset\u001b[1;34m(input_file, output_file)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Save augmented datasets\u001b[39;00m\n\u001b[0;32m     64\u001b[0m h5_out\u001b[38;5;241m.\u001b[39mcreate_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maugmented_inputs\u001b[39m\u001b[38;5;124m'\u001b[39m, data\u001b[38;5;241m=\u001b[39maugmented_inputs, compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m \u001b[43mh5_out\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maugmented_moves\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugmented_moves\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAugmented inputs shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maugmented_inputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAugmented dataset saved successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\aartienv\\Lib\\site-packages\\h5py\\_hl\\group.py:183\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[1;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[0;32m    180\u001b[0m         parent_path, name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    181\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[1;32m--> 183\u001b[0m dsid \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_new_dset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "File \u001b[1;32mD:\\aartienv\\Lib\\site-packages\\h5py\\_hl\\dataset.py:168\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[1;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0, fill_time)\u001b[0m\n\u001b[0;32m    165\u001b[0m dset_id \u001b[38;5;241m=\u001b[39m h5d\u001b[38;5;241m.\u001b[39mcreate(parent\u001b[38;5;241m.\u001b[39mid, name, tid, sid, dcpl\u001b[38;5;241m=\u001b[39mdcpl, dapl\u001b[38;5;241m=\u001b[39mdapl)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Empty)):\n\u001b[1;32m--> 168\u001b[0m     \u001b[43mdset_id\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh5s\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh5s\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset_id\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def augment_chess_dataset(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Augment chess dataset by flipping and rotating the input data.\n",
    "    The original and augmented data are saved into a new HDF5 file.\n",
    "\n",
    "    :param input_file: Path to the input HDF5 file\n",
    "    :param output_file: Path to the output HDF5 file\n",
    "    \"\"\"\n",
    "    with h5py.File(input_file, 'r') as h5_in, h5py.File(output_file, 'w') as h5_out:\n",
    "        # Copy original datasets to the output file\n",
    "        for key in h5_in.keys():\n",
    "            h5_out.create_dataset(key, data=h5_in[key])\n",
    "        \n",
    "        # Augment the 'inputs' dataset (chess boards)\n",
    "        inputs = h5_in['inputs'][:]\n",
    "        moves = h5_in['moves'][:]  # Preserve moves (labels)\n",
    "        \n",
    "        print(f\"Original inputs shape: {inputs.shape}\")\n",
    "        \n",
    "        augmented_inputs = []\n",
    "        augmented_moves = []\n",
    "\n",
    "        for i in range(inputs.shape[0]):\n",
    "            board = inputs[i]\n",
    "            move = moves[i]\n",
    "\n",
    "            # Original\n",
    "            augmented_inputs.append(board)\n",
    "            augmented_moves.append(move)\n",
    "\n",
    "            # Horizontal flip\n",
    "            flipped_h = np.flip(board, axis=1)\n",
    "            augmented_inputs.append(flipped_h)\n",
    "            augmented_moves.append(move)\n",
    "\n",
    "            # Vertical flip\n",
    "            flipped_v = np.flip(board, axis=0)\n",
    "            augmented_inputs.append(flipped_v)\n",
    "            augmented_moves.append(move)\n",
    "\n",
    "            # Rotate 90 degrees\n",
    "            rotated_90 = np.rot90(board, k=1, axes=(0, 1))\n",
    "            augmented_inputs.append(rotated_90)\n",
    "            augmented_moves.append(move)\n",
    "\n",
    "            # Rotate 180 degrees\n",
    "            rotated_180 = np.rot90(board, k=2, axes=(0, 1))\n",
    "            augmented_inputs.append(rotated_180)\n",
    "            augmented_moves.append(move)\n",
    "\n",
    "            # Rotate 270 degrees\n",
    "            rotated_270 = np.rot90(board, k=3, axes=(0, 1))\n",
    "            augmented_inputs.append(rotated_270)\n",
    "            augmented_moves.append(move)\n",
    "\n",
    "        # Convert augmented data to numpy arrays\n",
    "        augmented_inputs = np.array(augmented_inputs)\n",
    "        augmented_moves = np.array(augmented_moves)\n",
    "\n",
    "        # Save augmented datasets\n",
    "        h5_out.create_dataset('augmented_inputs', data=augmented_inputs, compression=\"gzip\")\n",
    "        h5_out.create_dataset('augmented_moves', data=augmented_moves, compression=\"gzip\")\n",
    "\n",
    "        print(f\"Augmented inputs shape: {augmented_inputs.shape}\")\n",
    "        print(\"Augmented dataset saved successfully.\")\n",
    "\n",
    "# Example usage\n",
    "input_file_path = r\"D:\\Aarti\\Dataset\\Chess_dataset4.h5\"\n",
    "output_file_path = r\"D:\\Aarti\\Dataset\\Chess_data_augmented3.h5\"\n",
    "\n",
    "augment_chess_dataset(input_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fc8bddb-7f7b-48d1-a346-7d1a76702b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to D:\\Aarti\\Dataset\\Processed_Chess_Data3.h5\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def process_and_save_hdf5(input_file_path, output_file_path):\n",
    "    \"\"\"\n",
    "    Process an HDF5 file to extract specific keys and save them to a new file.\n",
    "    \"\"\"\n",
    "    with h5py.File(input_file_path, 'r') as h5file:\n",
    "        # Check for the presence of required keys\n",
    "        required_keys = ['inputs', 'moves', 'evals']\n",
    "        for key in required_keys:\n",
    "            if key not in h5file:\n",
    "                raise KeyError(f\"Key '{key}' not found in the input HDF5 file!\")\n",
    "\n",
    "        # Extract data for the required keys\n",
    "        inputs = h5file['inputs'][:]\n",
    "        moves = h5file['moves'][:]\n",
    "        evals = h5file['evals'][:]\n",
    "\n",
    "        # Perform any necessary processing (if needed)\n",
    "        # Example: Normalize inputs or apply transformations\n",
    "        augmented_inputs = inputs  # Replace with any desired augmentation logic\n",
    "\n",
    "        # Save the processed data into a new HDF5 file\n",
    "        with h5py.File(output_file_path, 'w') as output_file:\n",
    "            output_file.create_dataset('augmented_inputs', data=augmented_inputs)\n",
    "            output_file.create_dataset('moves', data=moves)\n",
    "            output_file.create_dataset('evals', data=evals)\n",
    "\n",
    "    print(f\"Processed data saved to {output_file_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_h5_file_path = r\"D:\\Aarti\\Dataset\\Chess_data_augmented3.h5\"\n",
    "output_h5_file_path = r\"D:\\Aarti\\Dataset\\Processed_Chess_Data3.h5\"\n",
    "\n",
    "process_and_save_hdf5(input_h5_file_path, output_h5_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "707dd92b-dd85-4278-82c1-9686f671d271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to Chess_dataset_final10_eval.h5\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def merge_h5_files(file1, file2, output_file):\n",
    "    with h5py.File(file1, 'r') as hf1, h5py.File(file2, 'r') as hf2:\n",
    "        # Read datasets from both files\n",
    "        inputs1 = hf1['inputs'][:]\n",
    "        inputs2 = hf2['inputs'][:]\n",
    "        \n",
    "        \n",
    "        eval1 = hf1['evals'][:]\n",
    "        eval2 = hf2['evals'][:]\n",
    "        \n",
    "        # Concatenate datasets\n",
    "        inputs = np.concatenate((inputs1, inputs2), axis=0)\n",
    "      \n",
    "        evals = np.concatenate((eval1, eval2), axis=0)\n",
    "        \n",
    "        # Write merged data to new HDF5 file\n",
    "        with h5py.File(output_file, 'w') as hf_out:\n",
    "            hf_out.create_dataset('inputs', data=inputs)\n",
    "           \n",
    "            hf_out.create_dataset('evals', data=evals)\n",
    "        \n",
    "    print(f\"Merged data saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "file1 = r'D:\\\\Aarti\\\\Dataset\\\\Chess_dataset18_eval.h5'  # Replace with your first HDF5 file\n",
    "file2 = r'Chess_dataset_final9_eval.h5'  # Replace with your second HDF5 file\n",
    "output_file = 'Chess_dataset_final10_eval.h5'  # Output file for merged data\n",
    "\n",
    "merge_h5_files(file1, file2, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff69eb5-f589-4ca0-bf3b-d22648c7b048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
